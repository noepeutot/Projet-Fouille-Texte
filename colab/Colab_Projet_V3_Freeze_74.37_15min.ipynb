{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üçΩÔ∏è Projet Fouille d'Opinions - V3 FREEZE (~3x plus rapide)\n",
        "\n",
        "**Changement principal :** L'encodeur CamemBERT est gel√©, seules les t√™tes de classification sont entra√Æn√©es.\n",
        "\n",
        "| Version | Temps | Accuracy |\n",
        "|---------|-------|----------|\n",
        "| V2 (large, full) | ~50 min | 88.72% |\n",
        "| **V3 (large, freeze)** | **~15 min** | ~85-87% |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"model_name\": \"camembert/camembert-large\",\n",
        "    \"num_epochs\": 5,\n",
        "    \"batch_size\": 16,       # Plus grand car moins de m√©moire utilis√©e\n",
        "    \"learning_rate\": 1e-3,  # Plus grand car on n'entra√Æne que les t√™tes\n",
        "    \"max_length\": 256,\n",
        "    \"dropout\": 0.2,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"freeze_encoder\": True,  # ‚ö° NOUVEAU: Geler l'encodeur\n",
        "    \"patience\": 2,\n",
        "}\n",
        "print(\"üìã Config:\", CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "print(\"Uploadez ftdataset_train.tsv et ftdataset_val.tsv:\")\n",
        "for f in files.upload().keys():\n",
        "    os.rename(f, f'/content/data/{f}')\n",
        "!ls /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_utils"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "LABEL_TO_IDX = {\"Positive\": 0, \"N√©gative\": 1, \"Neutre\": 2, \"NE\": 3}\n",
        "IDX_TO_LABEL = {v: k for k, v in LABEL_TO_IDX.items()}\n",
        "ASPECTS = [\"Prix\", \"Cuisine\", \"Service\"]\n",
        "\n",
        "class OpinionDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, labels=None, max_length=256):\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
        "        self.labels = labels\n",
        "    def __len__(self): return len(self.encodings[\"input_ids\"])\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\"input_ids\": self.encodings[\"input_ids\"][idx], \"attention_mask\": self.encodings[\"attention_mask\"][idx]}\n",
        "        if self.labels:\n",
        "            for a in ASPECTS: item[f\"label_{a.lower()}\"] = torch.tensor(self.labels[a][idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "def prepare_labels(data): return {a: [LABEL_TO_IDX.get(d[a], 3) for d in data] for a in ASPECTS}\n",
        "def get_texts(data): return [d[\"Avis\"] for d in data]\n",
        "def collate_fn(f): \n",
        "    b = {\"input_ids\": torch.stack([x[\"input_ids\"] for x in f]), \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in f])}\n",
        "    for a in ASPECTS:\n",
        "        k = f\"label_{a.lower()}\"\n",
        "        if k in f[0]: b[k] = torch.stack([x[k] for x in f])\n",
        "    return b\n",
        "\n",
        "print(\"‚úÖ Data utils OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class FreezeClassifier(nn.Module):\n",
        "    def __init__(self, model_name, hidden_dim=256, dropout=0.2, freeze_encoder=True):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        \n",
        "        # ‚ö° FREEZE: Geler l'encodeur\n",
        "        if freeze_encoder:\n",
        "            for param in self.encoder.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"üîí Encodeur GEL√â - seules les t√™tes sont entra√Æn√©es\")\n",
        "        \n",
        "        hidden_size = self.encoder.config.hidden_size\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            a: nn.Sequential(\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_size, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, 4)\n",
        "            ) for a in ASPECTS\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with torch.no_grad() if not self.encoder.training else torch.enable_grad():\n",
        "            out = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls = out.last_hidden_state[:, 0, :]\n",
        "        return {a: self.classifiers[a](cls) for a in ASPECTS}\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"üîß Chargement de {config['model_name']}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "        self.model = FreezeClassifier(\n",
        "            config['model_name'], config['hidden_dim'], \n",
        "            config['dropout'], config['freeze_encoder']\n",
        "        ).to(self.device)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        total = sum(p.numel() for p in self.model.parameters())\n",
        "        print(f\"‚úÖ Mod√®le sur {self.device}\")\n",
        "        print(f\"   Param√®tres entra√Ænables: {trainable/1e6:.2f}M / {total/1e6:.1f}M\")\n",
        "\n",
        "    def train(self, train_data, val_data):\n",
        "        cfg = self.config\n",
        "        train_ds = OpinionDataset(get_texts(train_data), self.tokenizer, prepare_labels(train_data), cfg['max_length'])\n",
        "        val_ds = OpinionDataset(get_texts(val_data), self.tokenizer, prepare_labels(val_data), cfg['max_length'])\n",
        "        train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "        val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
        "        \n",
        "        optimizer = AdamW(filter(lambda p: p.requires_grad, self.model.parameters()), lr=cfg['learning_rate'])\n",
        "        \n",
        "        best_acc, best_state, patience = 0, None, 0\n",
        "        history = {'loss': [], 'acc': []}\n",
        "        \n",
        "        for epoch in range(cfg['num_epochs']):\n",
        "            print(f\"\\n--- Epoch {epoch+1}/{cfg['num_epochs']} ---\")\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "                ids, mask = batch[\"input_ids\"].to(self.device), batch[\"attention_mask\"].to(self.device)\n",
        "                logits = self.model(ids, mask)\n",
        "                loss = sum(self.criterion(logits[a], batch[f\"label_{a.lower()}\"].to(self.device)) for a in ASPECTS)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            history['loss'].append(avg_loss)\n",
        "            print(f\"Loss: {avg_loss:.4f}\")\n",
        "            \n",
        "            # Validation\n",
        "            acc, details = self._eval(val_loader)\n",
        "            history['acc'].append(acc)\n",
        "            print(f\"Val Acc: {acc:.2f}% | {details}\")\n",
        "            \n",
        "            if acc > best_acc:\n",
        "                best_acc, best_state, patience = acc, {k: v.cpu().clone() for k, v in self.model.state_dict().items()}, 0\n",
        "                print(\"‚≠ê Best!\")\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience >= cfg['patience']: \n",
        "                    print(\"‚ö†Ô∏è Early stop\")\n",
        "                    break\n",
        "        \n",
        "        if best_state: self.model.load_state_dict(best_state); self.model.to(self.device)\n",
        "        print(f\"\\nüèÜ BEST: {best_acc:.2f}%\")\n",
        "        return history, best_acc\n",
        "\n",
        "    def _eval(self, loader):\n",
        "        self.model.eval()\n",
        "        correct = {a: 0 for a in ASPECTS}\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in loader:\n",
        "                ids, mask = batch[\"input_ids\"].to(self.device), batch[\"attention_mask\"].to(self.device)\n",
        "                logits = self.model(ids, mask)\n",
        "                for a in ASPECTS:\n",
        "                    preds = torch.argmax(logits[a], dim=-1)\n",
        "                    correct[a] += (preds == batch[f\"label_{a.lower()}\"].to(self.device)).sum().item()\n",
        "                total += ids.size(0)\n",
        "        details = {a: round(100*correct[a]/total, 1) for a in ASPECTS}\n",
        "        return sum(details.values())/3, details\n",
        "\n",
        "    def predict(self, texts):\n",
        "        self.model.eval()\n",
        "        preds = []\n",
        "        for i in range(0, len(texts), 32):\n",
        "            enc = self.tokenizer(texts[i:i+32], truncation=True, padding=True, max_length=self.config['max_length'], return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(enc[\"input_ids\"].to(self.device), enc[\"attention_mask\"].to(self.device))\n",
        "            for j in range(len(texts[i:i+32])):\n",
        "                preds.append({a: IDX_TO_LABEL[torch.argmax(logits[a][j]).item()] for a in ASPECTS})\n",
        "        return preds\n",
        "\n",
        "print(\"‚úÖ Mod√®le OK\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv(\"/content/data/ftdataset_train.tsv\", sep=' *\\t *', encoding='utf-8', engine='python')\n",
        "df_val = pd.read_csv(\"/content/data/ftdataset_val.tsv\", sep=' *\\t *', encoding='utf-8', engine='python')\n",
        "train_data, val_data = df_train.to_dict('records'), df_val.to_dict('records')\n",
        "print(f\"‚úÖ Train={len(train_data)}, Val={len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(CONFIG)\n",
        "history, best_acc = trainer.train(train_data, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eval"
      },
      "outputs": [],
      "source": [
        "print(\"üìà √âvaluation finale...\")\n",
        "preds = trainer.predict(get_texts(val_data))\n",
        "correct = {a: sum(1 for p, r in zip(preds, val_data) if p[a] == r[a]) for a in ASPECTS}\n",
        "n = len(val_data)\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "for a in ASPECTS: print(f\"  {a}: {100*correct[a]/n:.2f}%\")\n",
        "print(f\"\\nüéØ MACRO: {sum(100*correct[a]/n for a in ASPECTS)/3:.2f}%\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save"
      },
      "outputs": [],
      "source": [
        "torch.save({'model': trainer.model.state_dict(), 'config': CONFIG, 'acc': best_acc}, '/content/model_v3_freeze.pt')\n",
        "print(f\"‚úÖ Sauvegard√© (acc: {best_acc:.2f}%)\")\n",
        "from google.colab import files\n",
        "files.download('/content/model_v3_freeze.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {"gpuType": "T4", "provenance": []},
    "kernelspec": {"display_name": "Python 3", "name": "python3"}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
