{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üçΩÔ∏è Projet Fouille d'Opinions - Version OPTIMIS√âE V2\n",
        "\n",
        "**Am√©liorations par rapport √† V1 (83.83%) :**\n",
        "- ‚úÖ **5 epochs** au lieu de 3\n",
        "- ‚úÖ **Mean pooling** au lieu du token CLS\n",
        "- ‚úÖ **Couche cach√©e suppl√©mentaire** (768 ‚Üí 256 ‚Üí 4)\n",
        "- ‚úÖ **Cosine scheduler** avec warmup\n",
        "- ‚úÖ **Label smoothing** (0.1)\n",
        "- ‚úÖ **Gradient clipping** pour stabilit√©\n",
        "- ‚úÖ **Early stopping** sur validation\n",
        "- ‚úÖ Option **camembert-large** pour meilleure performance\n",
        "\n",
        "**Objectif : atteindre ~87-92% d'accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## 1. Installation et v√©rification GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_section"
      },
      "source": [
        "## 2. Configuration - MODIFIEZ ICI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIGURATION - Modifiez ces valeurs pour exp√©rimenter\n",
        "# ============================================\n",
        "\n",
        "CONFIG = {\n",
        "    # Mod√®le - Options: \"camembert-base\", \"camembert/camembert-large\", \"flaubert/flaubert_base_cased\"\n",
        "    \"model_name\": \"camembert-base\",  # Changez en \"camembert/camembert-large\" pour plus de performance\n",
        "    \n",
        "    # Hyperparam√®tres d'entra√Ænement\n",
        "    \"num_epochs\": 5,           # Plus d'epochs = meilleur apprentissage\n",
        "    \"batch_size\": 16,          # R√©duire √† 8 si OOM avec camembert-large\n",
        "    \"learning_rate\": 2e-5,     # Taux d'apprentissage\n",
        "    \"max_length\": 256,         # Longueur max des textes\n",
        "    \n",
        "    # R√©gularisation\n",
        "    \"dropout\": 0.2,            # Dropout (augment√© de 0.1)\n",
        "    \"label_smoothing\": 0.1,    # Label smoothing pour r√©duire overfitting\n",
        "    \"weight_decay\": 0.01,      # L2 regularization\n",
        "    \"max_grad_norm\": 1.0,      # Gradient clipping\n",
        "    \n",
        "    # Scheduler\n",
        "    \"warmup_ratio\": 0.1,       # Warmup steps ratio\n",
        "    \"scheduler_type\": \"cosine\", # \"linear\" ou \"cosine\"\n",
        "    \n",
        "    # Architecture\n",
        "    \"use_mean_pooling\": True,  # Mean pooling au lieu de CLS\n",
        "    \"hidden_dim\": 256,         # Couche cach√©e interm√©diaire\n",
        "    \n",
        "    # Early stopping\n",
        "    \"patience\": 2,             # Arr√™ter si pas d'am√©lioration pendant N epochs\n",
        "}\n",
        "\n",
        "print(\"üìã Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## 3. Upload des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_files"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "\n",
        "print(\"Uploadez vos fichiers de donn√©es (ftdataset_train.tsv et ftdataset_val.tsv):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, f'/content/data/{filename}')\n",
        "    print(f\"‚úÖ {filename} d√©plac√© vers /content/data/\")\n",
        "\n",
        "!ls -la /content/data/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_section"
      },
      "source": [
        "## 4. Utilitaires de donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_utils"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "LABEL_TO_IDX = {\"Positive\": 0, \"N√©gative\": 1, \"Neutre\": 2, \"NE\": 3}\n",
        "IDX_TO_LABEL = {v: k for k, v in LABEL_TO_IDX.items()}\n",
        "ASPECTS = [\"Prix\", \"Cuisine\", \"Service\"]\n",
        "\n",
        "\n",
        "class OpinionDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, labels=None, max_length=256):\n",
        "        self.encodings = tokenizer(\n",
        "            texts, truncation=True, padding=True,\n",
        "            max_length=max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
        "        }\n",
        "        if self.labels:\n",
        "            for aspect in ASPECTS:\n",
        "                item[f\"label_{aspect.lower()}\"] = torch.tensor(self.labels[aspect][idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "def prepare_labels(data):\n",
        "    labels = {aspect: [] for aspect in ASPECTS}\n",
        "    for item in data:\n",
        "        for aspect in ASPECTS:\n",
        "            labels[aspect].append(LABEL_TO_IDX.get(item[aspect], 3))\n",
        "    return labels\n",
        "\n",
        "\n",
        "def get_texts(data):\n",
        "    return [item[\"Avis\"] for item in data]\n",
        "\n",
        "\n",
        "def collate_fn(features):\n",
        "    batch = {\n",
        "        \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
        "        \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
        "    }\n",
        "    for aspect in ASPECTS:\n",
        "        key = f\"label_{aspect.lower()}\"\n",
        "        if key in features[0]:\n",
        "            batch[key] = torch.stack([f[key] for f in features])\n",
        "    return batch\n",
        "\n",
        "\n",
        "print(\"‚úÖ Utilitaires de donn√©es charg√©s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_section"
      },
      "source": [
        "## 5. Mod√®le OPTIMIS√â"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_definition"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "import math\n",
        "\n",
        "\n",
        "class OptimizedClassifier(nn.Module):\n",
        "    \"\"\"Classificateur optimis√© avec mean pooling et couche cach√©e.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name, num_classes=4, hidden_dim=256, dropout=0.2, use_mean_pooling=True):\n",
        "        super().__init__()\n",
        "        self.config = AutoConfig.from_pretrained(model_name)\n",
        "        self.encoder = AutoModel.from_pretrained(model_name)\n",
        "        self.use_mean_pooling = use_mean_pooling\n",
        "\n",
        "        hidden_size = self.config.hidden_size\n",
        "\n",
        "        # T√™tes de classification avec couche cach√©e\n",
        "        self.classifiers = nn.ModuleDict({\n",
        "            aspect: nn.Sequential(\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_size, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            ) for aspect in ASPECTS\n",
        "        })\n",
        "\n",
        "    def mean_pooling(self, token_embeddings, attention_mask):\n",
        "        \"\"\"Mean pooling sur les tokens (meilleur que CLS pour la classification).\"\"\"\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        if self.use_mean_pooling:\n",
        "            pooled = self.mean_pooling(outputs.last_hidden_state, attention_mask)\n",
        "        else:\n",
        "            pooled = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        return {aspect: self.classifiers[aspect](pooled) for aspect in ASPECTS}\n",
        "\n",
        "\n",
        "class OptimizedTrainer:\n",
        "    \"\"\"Trainer optimis√© avec toutes les am√©liorations.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print(f\"üîß Chargement de {config['model_name']}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config['model_name'])\n",
        "        self.model = OptimizedClassifier(\n",
        "            model_name=config['model_name'],\n",
        "            hidden_dim=config['hidden_dim'],\n",
        "            dropout=config['dropout'],\n",
        "            use_mean_pooling=config['use_mean_pooling']\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Label smoothing cross entropy\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=config['label_smoothing'])\n",
        "\n",
        "        # Compter les param√®tres\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        print(f\"‚úÖ Mod√®le charg√© sur {self.device}\")\n",
        "        print(f\"   Param√®tres: {total_params/1e6:.1f}M (trainable: {trainable_params/1e6:.1f}M)\")\n",
        "\n",
        "    def train(self, train_data, val_data):\n",
        "        config = self.config\n",
        "        print(f\"\\nüìä Entra√Ænement sur {self.device}\")\n",
        "        print(f\"   Train: {len(train_data)} | Val: {len(val_data)}\")\n",
        "\n",
        "        # Datasets\n",
        "        train_dataset = OpinionDataset(\n",
        "            get_texts(train_data), self.tokenizer,\n",
        "            labels=prepare_labels(train_data), max_length=config['max_length']\n",
        "        )\n",
        "        val_dataset = OpinionDataset(\n",
        "            get_texts(val_data), self.tokenizer,\n",
        "            labels=prepare_labels(val_data), max_length=config['max_length']\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, collate_fn=collate_fn)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "        # Optimizer avec weight decay\n",
        "        optimizer = AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=config['learning_rate'],\n",
        "            weight_decay=config['weight_decay']\n",
        "        )\n",
        "\n",
        "        # Scheduler\n",
        "        total_steps = len(train_loader) * config['num_epochs']\n",
        "        warmup_steps = int(total_steps * config['warmup_ratio'])\n",
        "\n",
        "        if config['scheduler_type'] == 'cosine':\n",
        "            scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "        else:\n",
        "            scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "        # Training loop avec early stopping\n",
        "        best_val_acc = 0.0\n",
        "        best_model_state = None\n",
        "        patience_counter = 0\n",
        "        history = {'train_loss': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(config['num_epochs']):\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Epoch {epoch + 1}/{config['num_epochs']}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            # Training\n",
        "            self.model.train()\n",
        "            total_loss = 0.0\n",
        "            progress = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "            for batch in progress:\n",
        "                input_ids = batch[\"input_ids\"].to(self.device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "\n",
        "                logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "                loss = sum(\n",
        "                    self.criterion(logits[aspect], batch[f\"label_{aspect.lower()}\"].to(self.device))\n",
        "                    for aspect in ASPECTS\n",
        "                )\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), config['max_grad_norm'])\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                progress.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
        "\n",
        "            avg_loss = total_loss / len(train_loader)\n",
        "            history['train_loss'].append(avg_loss)\n",
        "            print(f\"\\nüìâ Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "            # Validation\n",
        "            val_acc, val_details = self._evaluate(val_loader)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            print(f\"\\nüìä Validation Accuracy: {val_acc:.2f}%\")\n",
        "            for aspect, acc in val_details.items():\n",
        "                print(f\"   {aspect}: {acc:.2f}%\")\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_model_state = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}\n",
        "                patience_counter = 0\n",
        "                print(f\"   ‚≠ê Nouveau meilleur mod√®le!\")\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                print(f\"   ‚è≥ Patience: {patience_counter}/{config['patience']}\")\n",
        "\n",
        "            if patience_counter >= config['patience']:\n",
        "                print(f\"\\n‚ö†Ô∏è Early stopping √† l'epoch {epoch + 1}\")\n",
        "                break\n",
        "\n",
        "        # Restaurer le meilleur mod√®le\n",
        "        if best_model_state:\n",
        "            self.model.load_state_dict(best_model_state)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"üèÜ MEILLEURE ACCURACY: {best_val_acc:.2f}%\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        return history, best_val_acc\n",
        "\n",
        "    def _evaluate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        correct = {aspect: 0 for aspect in ASPECTS}\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                input_ids = batch[\"input_ids\"].to(self.device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
        "                logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "                for aspect in ASPECTS:\n",
        "                    labels = batch[f\"label_{aspect.lower()}\"].to(self.device)\n",
        "                    preds = torch.argmax(logits[aspect], dim=-1)\n",
        "                    correct[aspect] += (preds == labels).sum().item()\n",
        "\n",
        "                total += input_ids.size(0)\n",
        "\n",
        "        details = {aspect: 100 * correct[aspect] / total for aspect in ASPECTS}\n",
        "        avg_acc = sum(details.values()) / len(ASPECTS)\n",
        "        return avg_acc, details\n",
        "\n",
        "    def predict(self, texts):\n",
        "        self.model.eval()\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(0, len(texts), 32):\n",
        "            batch_texts = texts[i:i+32]\n",
        "            encodings = self.tokenizer(\n",
        "                batch_texts, truncation=True, padding=True,\n",
        "                max_length=self.config['max_length'], return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_ids = encodings[\"input_ids\"].to(self.device)\n",
        "            attention_mask = encodings[\"attention_mask\"].to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "            for j in range(len(batch_texts)):\n",
        "                pred = {aspect: IDX_TO_LABEL[torch.argmax(logits[aspect][j]).item()] for aspect in ASPECTS}\n",
        "                predictions.append(pred)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "\n",
        "print(\"‚úÖ Mod√®le optimis√© d√©fini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_data_section"
      },
      "source": [
        "## 6. Chargement des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(\"/content/data/ftdataset_train.tsv\", sep=' *\\t *', encoding='utf-8', engine='python')\n",
        "df_val = pd.read_csv(\"/content/data/ftdataset_val.tsv\", sep=' *\\t *', encoding='utf-8', engine='python')\n",
        "\n",
        "train_data = df_train.to_dict(orient='records')\n",
        "val_data = df_val.to_dict(orient='records')\n",
        "\n",
        "print(f\"‚úÖ Donn√©es charg√©es: Train={len(train_data)}, Val={len(val_data)}\")\n",
        "\n",
        "# Afficher la distribution des classes\n",
        "print(\"\\nüìä Distribution des classes:\")\n",
        "for aspect in ASPECTS:\n",
        "    counts = df_train[aspect].value_counts()\n",
        "    print(f\"  {aspect}: {dict(counts)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_section"
      },
      "source": [
        "## 7. Entra√Ænement üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training"
      },
      "outputs": [],
      "source": [
        "# Cr√©er et entra√Æner le mod√®le\n",
        "trainer = OptimizedTrainer(CONFIG)\n",
        "history, best_acc = trainer.train(train_data, val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viz_section"
      },
      "source": [
        "## 8. Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "axes[0].plot(history['train_loss'], 'b-', marker='o')\n",
        "axes[0].set_title('Train Loss')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[0].grid(True)\n",
        "\n",
        "axes[1].plot(history['val_acc'], 'g-', marker='o')\n",
        "axes[1].set_title('Validation Accuracy')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy (%)')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/training_history.png', dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüìà Graphique sauvegard√© dans /content/training_history.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eval_section"
      },
      "source": [
        "## 9. √âvaluation finale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluation"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüìà √âvaluation finale...\")\n",
        "\n",
        "val_texts = get_texts(val_data)\n",
        "predictions = trainer.predict(val_texts)\n",
        "\n",
        "correct = {aspect: 0 for aspect in ASPECTS}\n",
        "n = len(val_data)\n",
        "\n",
        "for pred, ref in zip(predictions, val_data):\n",
        "    for aspect in ASPECTS:\n",
        "        if pred[aspect] == ref[aspect]:\n",
        "            correct[aspect] += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä R√âSULTATS FINAUX\")\n",
        "print(\"=\"*50)\n",
        "for aspect in ASPECTS:\n",
        "    acc = 100 * correct[aspect] / n\n",
        "    print(f\"  {aspect}: {acc:.2f}%\")\n",
        "\n",
        "macro_acc = sum(100 * correct[aspect] / n for aspect in ASPECTS) / len(ASPECTS)\n",
        "print(f\"\\nüéØ MACRO ACCURACY: {macro_acc:.2f}%\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "test_section"
      },
      "source": [
        "## 10. Test manuel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_examples"
      },
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    \"Excellente cuisine, plats savoureux et copieux. Le service √©tait un peu lent mais correct. Prix raisonnables.\",\n",
        "    \"Tr√®s d√©√ßu par ce restaurant. La nourriture √©tait froide et le serveur d√©sagr√©able. Bien trop cher.\",\n",
        "    \"Bon rapport qualit√©-prix. Service efficace et souriant. La cuisine √©tait correcte sans √™tre exceptionnelle.\",\n",
        "    \"Restaurant moyen, rien d'exceptionnel. Les prix sont corrects.\"\n",
        "]\n",
        "\n",
        "print(\"\\nüß™ Test sur quelques exemples:\\n\")\n",
        "predictions = trainer.predict(test_texts)\n",
        "\n",
        "for text, pred in zip(test_texts, predictions):\n",
        "    print(f\"üìù \\\"{text[:70]}...\\\"\")\n",
        "    print(f\"   ‚Üí Prix: {pred['Prix']}, Cuisine: {pred['Cuisine']}, Service: {pred['Service']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## 11. Sauvegarde du mod√®le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# Sauvegarder le mod√®le\n",
        "torch.save({\n",
        "    'model_state_dict': trainer.model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'best_accuracy': best_acc\n",
        "}, '/content/model_optimized_v2.pt')\n",
        "\n",
        "print(f\"‚úÖ Mod√®le sauvegard√© (accuracy: {best_acc:.2f}%)\")\n",
        "\n",
        "# T√©l√©charger\n",
        "from google.colab import files\n",
        "files.download('/content/model_optimized_v2.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_section"
      },
      "source": [
        "## 12. üî¨ Exp√©rimentations (Optionnel)\n",
        "\n",
        "Modifiez la configuration ci-dessus et relancez pour tester diff√©rentes combinaisons :\n",
        "\n",
        "**Suggestions :**\n",
        "1. `model_name: \"camembert/camembert-large\"` + `batch_size: 8`\n",
        "2. `num_epochs: 7` + `patience: 3`\n",
        "3. `learning_rate: 1e-5` (plus petit = plus stable)\n",
        "4. `hidden_dim: 512` (couche plus grande)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
