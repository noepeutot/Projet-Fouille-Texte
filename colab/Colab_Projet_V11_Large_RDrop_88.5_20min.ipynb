{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üçΩÔ∏è Projet V11 - CAMEMBERT-LARGE + R-DROP\n",
        "\n",
        "**Combinaison du meilleur mod√®le + meilleure technique**\n",
        "\n",
        "| Version | Mod√®le | Technique | Accuracy |\n",
        "|---------|--------|-----------|----------|\n",
        "| V9 | camembert-large | Mean Pooling | 88.44% |\n",
        "| V10 | camembertav2-base | R-Drop | 87.78% |\n",
        "| **V11** | **camembert-large** | **R-Drop** | **~89%+** |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers torch pandas numpy tqdm accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CONFIG V11 - CAMEMBERT-LARGE + R-DROP\n",
        "# ============================================\n",
        "CONFIG = {\n",
        "    \"model_name\": \"camembert/camembert-large\",  # 338M params\n",
        "    \"num_epochs\": 4,\n",
        "    \"batch_size\": 12,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"max_length\": 256,\n",
        "    \"dropout\": 0.1,\n",
        "    \"label_smoothing\": 0.1,\n",
        "    \"weight_decay\": 0.01,\n",
        "    \"warmup_ratio\": 0.1,\n",
        "    \"hidden_dim\": 384,\n",
        "    \"patience\": 2,\n",
        "    \"use_fp16\": True,\n",
        "    \"use_mean_pooling\": True,\n",
        "    \"use_rdrop\": True,\n",
        "    \"rdrop_alpha\": 0.5,  # Poids KL l√©g√®rement r√©duit\n",
        "}\n",
        "print(\"üìã Config V11:\")\n",
        "for k, v in CONFIG.items(): print(f\"  {k}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "print(\"Uploadez ftdataset_train.tsv et ftdataset_val.tsv:\")\n",
        "for f in files.upload().keys(): os.rename(f, f'/content/data/{f}')\n",
        "!ls /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch, torch.nn as nn, torch.nn.functional as F, pandas as pd, time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import autocast, GradScaler\n",
        "from transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "LABEL_TO_IDX = {\"Positive\": 0, \"N√©gative\": 1, \"Neutre\": 2, \"NE\": 3}\n",
        "IDX_TO_LABEL = {v: k for k, v in LABEL_TO_IDX.items()}\n",
        "ASPECTS = [\"Prix\", \"Cuisine\", \"Service\"]\n",
        "\n",
        "class DS(Dataset):\n",
        "    def __init__(s, t, tok, l=None, ml=256):\n",
        "        s.e = tok(t, truncation=True, padding=True, max_length=ml, return_tensors=\"pt\")\n",
        "        s.l = l\n",
        "    def __len__(s): return len(s.e[\"input_ids\"])\n",
        "    def __getitem__(s, i):\n",
        "        it = {\"input_ids\": s.e[\"input_ids\"][i], \"attention_mask\": s.e[\"attention_mask\"][i]}\n",
        "        if s.l:\n",
        "            for a in ASPECTS: it[f\"label_{a.lower()}\"] = torch.tensor(s.l[a][i], dtype=torch.long)\n",
        "        return it\n",
        "\n",
        "def prep_l(d): return {a: [LABEL_TO_IDX.get(x[a], 3) for x in d] for a in ASPECTS}\n",
        "def get_t(d): return [x[\"Avis\"] for x in d]\n",
        "def coll(f):\n",
        "    b = {\"input_ids\": torch.stack([x[\"input_ids\"] for x in f]), \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in f])}\n",
        "    for a in ASPECTS:\n",
        "        k = f\"label_{a.lower()}\"\n",
        "        if k in f[0]: b[k] = torch.stack([x[k] for x in f])\n",
        "    return b\n",
        "\n",
        "def kl_div_loss(p, q):\n",
        "    \"\"\"KL divergence sym√©trique pour R-Drop\"\"\"\n",
        "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='batchmean')\n",
        "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='batchmean')\n",
        "    return (p_loss + q_loss) / 2\n",
        "\n",
        "class ClassifierV11(nn.Module):\n",
        "    def __init__(s, mn, hd=384, dr=0.1, use_mean_pooling=True):\n",
        "        super().__init__()\n",
        "        s.enc = AutoModel.from_pretrained(mn)\n",
        "        hs = s.enc.config.hidden_size\n",
        "        s.use_mean_pooling = use_mean_pooling\n",
        "        \n",
        "        s.cls = nn.ModuleDict({\n",
        "            a: nn.Sequential(\n",
        "                nn.Dropout(dr),\n",
        "                nn.Linear(hs, hd),\n",
        "                nn.GELU(),\n",
        "                nn.LayerNorm(hd),\n",
        "                nn.Dropout(dr),\n",
        "                nn.Linear(hd, 4)\n",
        "            ) for a in ASPECTS\n",
        "        })\n",
        "    \n",
        "    def mean_pooling(s, hidden_states, attention_mask):\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "        return sum_embeddings / sum_mask\n",
        "    \n",
        "    def forward(s, ids, mask):\n",
        "        o = s.enc(input_ids=ids, attention_mask=mask)\n",
        "        if s.use_mean_pooling:\n",
        "            pooled = s.mean_pooling(o.last_hidden_state, mask)\n",
        "        else:\n",
        "            pooled = o.last_hidden_state[:, 0, :]\n",
        "        return {a: s.cls[a](pooled) for a in ASPECTS}\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(s, c):\n",
        "        s.c = c; s.dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"üîß Chargement de {c['model_name']}...\")\n",
        "        s.tok = AutoTokenizer.from_pretrained(c['model_name'])\n",
        "        s.m = ClassifierV11(\n",
        "            c['model_name'], c['hidden_dim'], c['dropout'], c['use_mean_pooling']\n",
        "        ).to(s.dev)\n",
        "        s.ce_crit = nn.CrossEntropyLoss(label_smoothing=c['label_smoothing'])\n",
        "        s.scaler = GradScaler('cuda') if c['use_fp16'] else None\n",
        "        params = sum(p.numel() for p in s.m.parameters())/1e6\n",
        "        print(f\"‚úÖ Mod√®le sur {s.dev} ({params:.0f}M params)\")\n",
        "        print(f\"   Mean Pooling: {c['use_mean_pooling']} | FP16: {c['use_fp16']} | R-Drop: {c['use_rdrop']}\")\n",
        "\n",
        "    def train(s, td, vd):\n",
        "        c = s.c; st = time.time()\n",
        "        tds = DS(get_t(td), s.tok, prep_l(td), c['max_length'])\n",
        "        vds = DS(get_t(vd), s.tok, prep_l(vd), c['max_length'])\n",
        "        tl = DataLoader(tds, batch_size=c['batch_size'], shuffle=True, collate_fn=coll)\n",
        "        vl = DataLoader(vds, batch_size=c['batch_size'], shuffle=False, collate_fn=coll)\n",
        "        \n",
        "        opt = AdamW(s.m.parameters(), lr=c['learning_rate'], weight_decay=c['weight_decay'])\n",
        "        ts = len(tl) * c['num_epochs']\n",
        "        sch = get_cosine_schedule_with_warmup(opt, int(ts * c['warmup_ratio']), ts)\n",
        "        \n",
        "        ba, bs, pa = 0, None, 0\n",
        "        \n",
        "        for ep in range(c['num_epochs']):\n",
        "            t0 = time.time()\n",
        "            print(f\"\\n{'='*60}\\nEpoch {ep+1}/{c['num_epochs']}\\n{'='*60}\")\n",
        "            s.m.train(); tls = 0; kls = 0\n",
        "            \n",
        "            for b in tqdm(tl, desc=\"Training\"):\n",
        "                ids, mask = b[\"input_ids\"].to(s.dev), b[\"attention_mask\"].to(s.dev)\n",
        "                opt.zero_grad()\n",
        "                \n",
        "                if c['use_fp16']:\n",
        "                    with autocast('cuda'):\n",
        "                        lo1 = s.m(ids, mask)\n",
        "                        ce_loss = sum(s.ce_crit(lo1[a], b[f\"label_{a.lower()}\"].to(s.dev)) for a in ASPECTS)\n",
        "                        \n",
        "                        if c['use_rdrop']:\n",
        "                            lo2 = s.m(ids, mask)\n",
        "                            ce_loss2 = sum(s.ce_crit(lo2[a], b[f\"label_{a.lower()}\"].to(s.dev)) for a in ASPECTS)\n",
        "                            kl_loss = sum(kl_div_loss(lo1[a], lo2[a]) for a in ASPECTS)\n",
        "                            loss = (ce_loss + ce_loss2) / 2 + c['rdrop_alpha'] * kl_loss\n",
        "                            kls += kl_loss.item()\n",
        "                        else:\n",
        "                            loss = ce_loss\n",
        "                    \n",
        "                    s.scaler.scale(loss).backward()\n",
        "                    s.scaler.unscale_(opt)\n",
        "                    torch.nn.utils.clip_grad_norm_(s.m.parameters(), 1.0)\n",
        "                    s.scaler.step(opt)\n",
        "                    s.scaler.update()\n",
        "                else:\n",
        "                    lo1 = s.m(ids, mask)\n",
        "                    ce_loss = sum(s.ce_crit(lo1[a], b[f\"label_{a.lower()}\"].to(s.dev)) for a in ASPECTS)\n",
        "                    if c['use_rdrop']:\n",
        "                        lo2 = s.m(ids, mask)\n",
        "                        ce_loss2 = sum(s.ce_crit(lo2[a], b[f\"label_{a.lower()}\"].to(s.dev)) for a in ASPECTS)\n",
        "                        kl_loss = sum(kl_div_loss(lo1[a], lo2[a]) for a in ASPECTS)\n",
        "                        loss = (ce_loss + ce_loss2) / 2 + c['rdrop_alpha'] * kl_loss\n",
        "                        kls += kl_loss.item()\n",
        "                    else:\n",
        "                        loss = ce_loss\n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(s.m.parameters(), 1.0)\n",
        "                    opt.step()\n",
        "                \n",
        "                sch.step()\n",
        "                tls += ce_loss.item()\n",
        "            \n",
        "            avg_ce = tls/len(tl)\n",
        "            avg_kl = kls/len(tl) if c['use_rdrop'] else 0\n",
        "            print(f\"CE Loss: {avg_ce:.4f}\" + (f\" | KL Loss: {avg_kl:.4f}\" if c['use_rdrop'] else \"\"))\n",
        "            \n",
        "            ac, dt = s._ev(vl)\n",
        "            print(f\"Val: {ac:.2f}% | {dt} | ‚è±Ô∏è {time.time()-t0:.0f}s\")\n",
        "            \n",
        "            if ac > ba:\n",
        "                ba, bs, pa = ac, {k: v.cpu().clone() for k, v in s.m.state_dict().items()}, 0\n",
        "                print(\"‚≠ê Best!\")\n",
        "            else:\n",
        "                pa += 1\n",
        "                print(f\"‚è≥ Patience: {pa}/{c['patience']}\")\n",
        "                if pa >= c['patience']: print(\"‚ö†Ô∏è Early stop\"); break\n",
        "        \n",
        "        if bs: s.m.load_state_dict(bs); s.m.to(s.dev)\n",
        "        tt = time.time() - st\n",
        "        print(f\"\\n{'='*60}\\nüèÜ BEST: {ba:.2f}% | ‚è±Ô∏è {tt/60:.1f} min\\n{'='*60}\")\n",
        "        return ba, tt\n",
        "\n",
        "    def _ev(s, ld):\n",
        "        s.m.eval(); cor, tot = {a: 0 for a in ASPECTS}, 0\n",
        "        with torch.no_grad():\n",
        "            for b in ld:\n",
        "                ids, mask = b[\"input_ids\"].to(s.dev), b[\"attention_mask\"].to(s.dev)\n",
        "                if s.c['use_fp16']:\n",
        "                    with autocast('cuda'):\n",
        "                        lo = s.m(ids, mask)\n",
        "                else:\n",
        "                    lo = s.m(ids, mask)\n",
        "                for a in ASPECTS:\n",
        "                    cor[a] += (torch.argmax(lo[a], -1) == b[f\"label_{a.lower()}\"].to(s.dev)).sum().item()\n",
        "                tot += ids.size(0)\n",
        "        dt = {a: round(100*cor[a]/tot, 1) for a in ASPECTS}\n",
        "        return sum(dt.values())/3, dt\n",
        "\n",
        "    def predict(s, t):\n",
        "        s.m.eval(); p = []\n",
        "        for i in range(0, len(t), 32):\n",
        "            e = s.tok(t[i:i+32], truncation=True, padding=True, max_length=s.c['max_length'], return_tensors=\"pt\")\n",
        "            with torch.no_grad():\n",
        "                if s.c['use_fp16']:\n",
        "                    with autocast('cuda'):\n",
        "                        lo = s.m(e[\"input_ids\"].to(s.dev), e[\"attention_mask\"].to(s.dev))\n",
        "                else:\n",
        "                    lo = s.m(e[\"input_ids\"].to(s.dev), e[\"attention_mask\"].to(s.dev))\n",
        "            for j in range(len(t[i:i+32])):\n",
        "                p.append({a: IDX_TO_LABEL[torch.argmax(lo[a][j]).item()] for a in ASPECTS})\n",
        "        return p\n",
        "\n",
        "print(\"‚úÖ Code V11 pr√™t!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"/content/data/ftdataset_train.tsv\", sep=' *\\t *', encoding='utf-8', engine='python')\n",
        "df_val = pd.read_csv(\"/content/data/ftdataset_val.tsv\", sep=' *\\t *', encoding='utf-8', engine='python')\n",
        "train_data, val_data = df_train.to_dict('records'), df_val.to_dict('records')\n",
        "print(f\"‚úÖ Train={len(train_data)}, Val={len(val_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer = Trainer(CONFIG)\n",
        "best_acc, total_time = trainer.train(train_data, val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüìà √âvaluation finale...\")\n",
        "preds = trainer.predict(get_t(val_data))\n",
        "correct = {a: sum(1 for p, r in zip(preds, val_data) if p[a] == r[a]) for a in ASPECTS}\n",
        "n = len(val_data)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä R√âSULTATS V11 (CAMEMBERT-LARGE + R-DROP)\")\n",
        "print(\"=\"*60)\n",
        "for a in ASPECTS: print(f\"  {a}: {100*correct[a]/n:.2f}%\")\n",
        "macro = sum(100*correct[a]/n for a in ASPECTS)/3\n",
        "print(f\"\\nüéØ MACRO: {macro:.2f}% | ‚è±Ô∏è {total_time/60:.1f} min\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìà Am√©lioration vs V9 (88.44%): {macro - 88.44:+.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save({'model': trainer.m.state_dict(), 'config': CONFIG, 'acc': best_acc}, '/content/model_v11.pt')\n",
        "print(f\"‚úÖ Sauvegard√© (acc: {best_acc:.2f}%)\")\n",
        "from google.colab import files\n",
        "files.download('/content/model_v11.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {"gpuType": "T4"},
    "kernelspec": {"display_name": "Python 3", "name": "python3"}
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
