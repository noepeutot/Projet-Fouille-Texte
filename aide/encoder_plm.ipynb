{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
    "\n",
    "plm_name = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(plm_name)\n",
    "lmtokenizer = AutoTokenizer.from_pretrained(plm_name)\n",
    "lm: AutoModel = AutoModel.from_pretrained(plm_name, output_attentions=False, add_pooling_layer=False, ignore_mismatched_sizes=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'learn', 'the', 'various', 'meanings', 'of', 'rec', '##omp', '##ilation', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "texts = [\n",
    "    \"Learn the various meanings of recompilation\",\n",
    "    \"This is another good example.\"\n",
    "]\n",
    "\n",
    "spec_flag = True\n",
    "print(lmtokenizer.tokenize(texts[0], add_special_tokens=True))\n",
    "# encoded_text = lmtokenizer(texts, add_special_tokens=True, padding=True, return_tensors='pt')\n",
    "\n",
    "# pprint(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the vocabulary of the model and print its size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = lmtokenizer.get_vocab()\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the id of the token `cat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4937"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what are the tokens stored at indexes 2000 to 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "was\n",
      "he\n",
      "is\n",
      "as\n",
      "for\n",
      "on\n",
      "with\n",
      "that\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "id2token = { id:token for token, id in vocab.items()}\n",
    "\n",
    "for i in range(2000, 2010):\n",
    "    print(id2token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the tokenizer on a couple of texts, and get the indices of the tokens (with padding, so that we have a matrix) and also the attention mask matrix: the result is dict with keys `input_ids` and `atention_mask`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 0, 0]]),\n",
      " 'input_ids': tensor([[ 4553,  1996,  2536, 15383,  1997, 28667, 25377, 29545],\n",
      "        [ 2023,  2003,  2178,  2204,  2742,  1012,     0,     0]])}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "texts = [\n",
    "    \"Learn the various meanings of recompilation\",\n",
    "    \"This is another good example.\"\n",
    "]\n",
    "\n",
    "spec_flag = True\n",
    "# print(lmtokenizer.tokenize(texts[0], add_special_tokens=True))\n",
    "\n",
    "encoded_text = lmtokenizer(texts, add_special_tokens=False, padding=True, return_tensors='pt', return_token_type_ids=False)\n",
    "\n",
    "pprint(encoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the LM on the input texts, by providing as arguments the `input_ids` and the `attention_mask`. This will return contextual embeddings, encoding each input token as a vector of dimension `d` (the dimension of the latent representations of the LM encoder). As expected, the result is a tensor of shape `(N, T, d)`, `N` being the batch size (number of input texts in the batch), `T` is the max length (max number of tokens) of the input texts, and `d` is the model embedding dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_output= lm(input_ids=encoded_text['input_ids'], attention_mask=encoded_text['attention_mask'])\n",
    "\n",
    "embs = lm_output.last_hidden_state\n",
    "\n",
    "embs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access the vector of the 4th token of the 2nd text and display its dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[1,3].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the mean vectors for each text (i.e. mean along the temporal dimension 1), so we will get a matrix, i.e. 2 vectors of dim `d`. Warning: this is not the best way of computing the mean embedding of texts because the embeddings of padding tokens (which are not real tokens of the input text) are included in the mean: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.mean(dim=1).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discard the padding tokens when computing the mean embeddings by msetting to zero all the values where mask=0, summing the non-zero values and dividing by the length of the token sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1553,  0.1637,  0.0750,  ...,  0.1541, -0.0194,  0.0119],\n",
       "        [ 0.0246, -0.3282,  0.0557,  ..., -0.1639,  0.3721,  0.2535]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "expanded_mask = encoded_text['attention_mask'].unsqueeze(-1).expand(embs.size())\n",
    "\n",
    "lengths = torch.clamp(expanded_mask.sum(-2), min=1e-12)\n",
    "\n",
    "(embs * expanded_mask).sum(dim=-2) / lengths\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
