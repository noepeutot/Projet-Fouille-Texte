{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLMs locally on a laptop/PC\n",
    "\n",
    "Quantized versions of LLMs can be run locally on a laptop or PC. Having at least 16GB of RAM and a GPU of 8GB of memory can help in terms of generation speed, but are not necessary: 8GB of RAM and a decent CPU can run the small versions of some LLMS.\n",
    "One of the easiest way to get LLMs work on a laptop or PC is to install Ollama (https://ollama.com/). Download the version that is compatible with your machine and install it. Ollama can then operate as an LLM server, and can be used either interactively through the CLI, or a compatible UI, or programmatically through the Ollama python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have installed ollama, you need to select a model and download it: open a terminal and type in the the following command to pull and download an LLM:\n",
    "\n",
    "```shell\n",
    "ollama pull model_name\n",
    "```\n",
    "\n",
    "The list of available LLMs in Ollama can be found in this page: https://ollama.com/search\n",
    "\n",
    "There are plenty of models you can try, but generally speaking, models from the llama3.2, Qwen2.5 or gemma3 families are good options. Here are examples of LLMs sorted by size (number of parameters), from 1.5 to 12 billions parameters:\n",
    "\n",
    "- Qwen2.5 1.5B: https://ollama.com/library/qwen2.5:1.5b\n",
    "- llama3.2 3B:  https://ollama.com/library/llama3.2:3b\n",
    "- gemma3 4B: https://ollama.com/library/gemma3:4b\n",
    "- qwen3:4b-instruct: https://ollama.com/library/qwen3:4b-instruct\n",
    "- Qwen2.5 7B: https://ollama.com/library/qwen2.5:7b\n",
    "- gemma3 12B: https://ollama.com/library/gemma3:12b\n",
    "\n",
    "Which LLM and which size to download depends on the available computer capabilities in terms of GPU, GPU memory, CPU and RAM size.\n",
    "\n",
    "The models can be downloaded with an ollama command. For instance, to download gemma3 4B, open a terminal and type in the following command:\n",
    "\n",
    "```ollama pull gemma3:4b```\n",
    "\n",
    "Once the downloading of the model has finished, you can use it either within the CLI, by typing in a terminal:\n",
    "\n",
    "\n",
    "```ollama run gemma3:4b```\n",
    "\n",
    "\n",
    "However, it is much more convenient to use a chat user interface (UI). Among those compatible with the Ollama server: `open-webui`, a very rich, open source UI. A simpler alternative is `Page Assist`, which is an open source browser extension available for Firefox-based browsers (https://addons.mozilla.org/en-US/firefox/addon/page-assist/) and Chrome-based browsers (https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatic use of ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the ollama API programmatically, you need to install the ollama server and launch it, and also install the ollama-python library (https://github.com/ollama/ollama-python):\n",
    "\n",
    "```shell\n",
    "pip install ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python, you have to create an ollama client first, which you will use to send requests to the ollama server.\n",
    "\n",
    "The following example shows how to access the local ollama server to get a list of available (installed) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(model='qwen2.5:7b', modified_at=datetime.datetime(2025, 10, 8, 17, 56, 16, 472887, tzinfo=TzInfo(7200)), digest='845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e', size=4683087332, details=ModelDetails(parent_model='', format='gguf', family='qwen2', families=['qwen2'], parameter_size='7.6B', quantization_level='Q4_K_M')),\n",
       " Model(model='gemma3:4b', modified_at=datetime.datetime(2025, 10, 8, 16, 30, 55, 385871, tzinfo=TzInfo(7200)), digest='a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a', size=3338801804, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='4.3B', quantization_level='Q4_K_M')),\n",
       " Model(model='gemma3:1b', modified_at=datetime.datetime(2025, 9, 23, 11, 18, 51, 513820, tzinfo=TzInfo(7200)), digest='8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc', size=815319791, details=ModelDetails(parent_model='', format='gguf', family='gemma3', families=['gemma3'], parameter_size='999.89M', quantization_level='Q4_K_M')),\n",
       " Model(model='llama3:latest', modified_at=datetime.datetime(2025, 9, 23, 11, 15, 25, 269816, tzinfo=TzInfo(7200)), digest='365c0bd3c000a25d28ddbf732fe1c6add414de7275464c4e4d1c3b5fcb5d8ad1', size=4661224676, details=ModelDetails(parent_model='', format='gguf', family='llama', families=['llama'], parameter_size='8.0B', quantization_level='Q4_0'))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# We assume that ollama has been installed and the ollama server already started (see https://ollama.com/)\n",
    "\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "# Get an ollama client\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "# Print the list of available models:\n",
    "llmclient.list()['models']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to send queries and get LLM responses is the `generate()` function, as shown in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================== LLM generated response:\n",
      " As of today, November 2, 2023, the French Prime Minister is **Gabriel Attal**. \n",
      "\n",
      "You can always find the most up-to-date information on this and other government positions here:\n",
      "\n",
      "*   **Official Website of the French Government:** [https://www.gouvernement.fr/](https://www.gouvernement.fr/)\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from ollama import Client\n",
    "\n",
    "ollama_url = 'http://localhost:11434'\n",
    "model_name = 'gemma3:4b'\n",
    "# model_name = 'qwen2.5:72b'\n",
    "\n",
    "# Get an ollama client\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 200,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "result = llmclient.generate(model=model_name, prompt='Who is currently the French Prime Minister?', options=model_options)\n",
    "\n",
    "# pprint(vars(result), compact=True, sort_dicts=False)\n",
    "\n",
    "print(\"\\n===================== LLM generated response:\\n\", result.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use the chat function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The blue color of the sky is a fascinating phenomenon caused by a process called **Rayleigh scattering**. Here's a breakdown of how it works:\n",
      "\n",
      "**1. Sunlight and Colors:**\n",
      "\n",
      "* Sunlight is actually made up of *all* the colors of the rainbow. We see this when light passes through a prism and separates into its different wavelengths.\n",
      "\n",
      "**2. Rayleigh Scattering:**\n",
      "\n",
      "* As sunlight enters the Earth's atmosphere, it collides with tiny air molecules (mostly nitrogen\n"
     ]
    }
   ],
   "source": [
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = llmclient.chat(\n",
    "    model=model_name, \n",
    "    messages=[ {'role': 'user', 'content': 'Why is the sky blue?'} ],\n",
    "    options={'num_predict': 100}\n",
    ")\n",
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream mode\n",
    "In the following code, the client retrieves the answer in a streaming mode. The streaming mode is useful when you want to display LLM answers in an interactive UI, so the user can immediately see the LLM answer while it is being fetched, instead of waiting for the full answer to be retrieved and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's an explanation of a transformer in a few sentences:\n",
      "\n",
      "A transformer is a device that uses electromagnetic induction to transfer electrical energy from one circuit to another with a different voltage. It consists of two or more coils of wire linked by a magnetic core.  By varying the number of turns in the coils, it can step up or step down the voltage efficiently, making it a crucial component in power distribution systems. \n",
      "\n",
      "---\n",
      "\n",
      "Would you like me to delve into a specific aspect of transformers, like their operation, types, or applications?"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "\n",
    "ollama_url = 'http://localhost:11434'\n",
    "model_name = 'gemma3:4b'\n",
    "# Get an ollama client\n",
    "llmclient = Client(ollama_url)\n",
    "\n",
    "prompt = \"\"\"Explain in a few sentences what is a transformer\"\"\"\n",
    "\n",
    "options={\n",
    "    'num_predict': 200,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "stream = llmclient.chat(\n",
    "  model=model_name,\n",
    "  messages=[{'role': 'user', 'content': prompt}],\n",
    "  options=options,\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic OpenAI Completion API\n",
    "\n",
    "Instead of using the ollama specific python API, it is possible to use the OpenAI API with the ollama server, provided that you have installed the openai python library. \n",
    "\n",
    "Creating an OpenAI client for the ollama server and running a query with the `completions` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE: As of my last update in October 2023, the President of France is Emmanuel Macron. He has been serving since May 14, 2017. However, please check for the most current information as presidential terms can be extended or changed over time.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "server_url = \"http://localhost:11434/v1\"\n",
    "model_name = \"qwen2.5:7b\"\n",
    "\n",
    "llmclient = OpenAI(\n",
    "    base_url=server_url,\n",
    "    api_key='EMPTY', # required, but not used\n",
    ")\n",
    "\n",
    "prompt = \"Who is the president of France?\"\n",
    "resp = llmclient.completions.create(\n",
    "            model=model_name,\n",
    "            prompt=prompt,\n",
    "            temperature=0.1, \n",
    "            top_p=0.1,\n",
    "            max_tokens=300,\n",
    "            extra_body=None,\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"RESPONSE:\", resp.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Chat function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same as above, but using the `chat` OpenAI API instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**. üòä \n",
      "\n",
      "Do you want to know anything more about Paris, or would you like to ask me another question?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url = \"http://localhost:11434/v1\"\n",
    "model_name = \"gemma3:4b\"\n",
    "\n",
    "# Configure the client to use th local Ollama server\n",
    "client = OpenAI(\n",
    "    base_url=base_url,  # Ollama's OpenAI-compatible endpoint\n",
    "    api_key=\"ollama\"  # Dummy key; Ollama does not require auth\n",
    ")\n",
    "\n",
    "# Make a chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,  # Or any other model you've pulled with `ollama pull`\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM-based text classification\n",
    "\n",
    "In this example, we ask the selected LLM (in French) to classify a text into one or more possible categories among the ones provided. We then get and print the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " [\"Energies renouvelables et nucl√©aires\", \"Engagement politique et entreprises\", \"Solution √©cologique innovante\"]\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "\n",
    "# Get an OpenAI client\n",
    "llmclient = OpenAI(\n",
    "    base_url=base_url,  \n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "prompt = \"\"\"Consid√©rez le texte suivant;\n",
    "\n",
    "\"Microsoft s'associe √† G42 pour lancer un projet colossal d'1 milliard de dollars : un centre de donn√©es √©cologique au Kenya ! Aliment√© par le potentiel inexploit√© des 10 gigawatts d'√©nergie g√©othermique du Kenya, le m√©ga centre de donn√©es est le premier de son genre\"\n",
    "\n",
    "\n",
    "Le texte fait-il partie des cat√©gories suivantes ? Si oui listez juste la ou les cat√©gories concern√©es sous forme de liste Python sans explications, sinon r√©pondez juste \"non\" :\n",
    "\n",
    "Liste de cat√©gories possibles : \"Activisme √©cologique\", \"Comportement consommateur\", \"Energies renouvelables et nucl√©aires\", \"Engagement politique et entreprises\", \"Reforestation\", \"Solution √©cologique innovante\", \"Transport d√©carbon√©\".\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that performs text classification.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "result = llmclient.chat.completions.create(\n",
    "    model=model_name, \n",
    "    messages= messages,\n",
    "    max_tokens=200,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9)\n",
    "\n",
    "print(\"LLM response:\\n\", result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opinion classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the HggingFace dataset library to download an Amazon product review dataset (only the part in French):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noepeutot/miniforge3/envs/fouille_texte/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label', 'label_text'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"SetFit/amazon_reviews_multi_fr\", split=\"validation\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a review sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'fr_0112905',\n",
      " 'label': 0,\n",
      " 'label_text': '0',\n",
      " 'text': 'Colis bien re√ßus avec le bo√Ætier du jeu ouvert, cass√© et sans le cd. '\n",
      "         'Pr√©command√© pour recevoir √ßa le jour de la sortie sa fait plaisir... '\n",
      "         'Et le service client Amazon qui cherche √† mettre la faute sur le '\n",
      "         'livreur qui a tr√®s bien fait sont travail en livrant un colis en '\n",
      "         'excellant √©tat... Cela m√©riterait m√™me un 0!'}\n"
     ]
    }
   ],
   "source": [
    "pprint(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we filter out the neutral reviews (i.e. reviews with rating score==2) and print the first 3 data samples, both the review texts and the labels. The labels are from 0 to 4 and correspond to the rating score given by the user to the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5000/5000 [00:00<00:00, 51062.87 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': ['fr_0820252', 'fr_0736148', 'fr_0032774'],\n",
       " 'text': ['J\\'ai achet√© ce c√¢ble display port pour mon nouvel √©cran 144hz 24\" afin d\\'en profiter au maximum. De prime abord le c√¢ble semblait de bonne qualit√©, une bonne tenue sur l\\'√©cran et le pc, avec une longueur plus que correcte pour relier mon √©cran √† ma tour. Seulement apr√®s quelques semaines, l\\'affichage n\\'arr√™te pas de \"sauter\", l\\'√©cran n\\'est plus d√©tect√© et n\\'affiche plus rien. Plus r√©cemment lorsque je regarde des vid√©os sur Youtube ou Netflix par exemple, l\\'image de la vid√©o devient floue par endroits, avec des reflets brouillons... Je suis plut√¥t d√©sappoint√©, je vais devoir en racheter un, √† moins qu\\'il y ait possibilit√© d\\'obtenir un produit de remplacement pour ce c√¢ble d√©fectueux.',\n",
       "  \"L'article re√ßu √©tait noir et non gris. Le c√¢ble s'est ab√Æm√© tr√®s rapidement. Assez d√©√ßue !\",\n",
       "  \"Je d√©nigre pas le produit mais je le d√©conseille aux personnes chez qui la perte de cheveux fait suite √† un traitement post- cancer. Pas d'effets, hormis sur les ongles (alors que les miens n'ont aucun probl√®me) mais la chute de cheveux persiste et aucun signe de repousse malgr√© plusieurs mois de traitement. J'avais beaucoup d'espoir en achetant ce produit, c'est le 3√®me que j'essaie et je suis tr√®s d√©√ßue.\"],\n",
       " 'label': [1, 1, 1],\n",
       " 'label_text': ['1', '1', '1']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.filter(lambda sample: sample['label'] != 2)\n",
    "ds = ds.shuffle(seed=42)\n",
    "ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for the classification of the reviews into positive or negative, using an LLM via the ollama library. We consider all the ratings equal or greater than 3 to be positive, and rating of 2 or below to be negative. The script also computes the classification precision (performance).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [01:44<00:00,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM=qwen2.5:7b, Classification Precision=95.00%  (19/20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "from tqdm import tqdm\n",
    "from ollama import Client\n",
    "\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "# model_name = 'llama3.1:8b'   # acc=91.00%  [00:42<00:00,  4.72it/s]\n",
    "# model_name = 'gemma2:2b'   # acc=91.00%   [00:33<00:00,  5.90it/s]\n",
    "# model_name = 'gemma2:9b'   # acc=92.00%   [01:36<00:00,  2.08it/s]\n",
    "model_name = 'qwen2.5:7b'    # acc=92.00% on first 200: [00:40<00:00,  4.90it/s]\n",
    "# model_name = 'qwen2.5:32b'  # acc=94% on first 200 samples  200/200 [11:54<00:00,  3.57s/it]\n",
    "# model_name = 'gemma3:4b'    # acc=92.50%  [00:38<00:00,  5.20it/s]\n",
    "# model_name = 'gemma3:12b'    # acc=96.00%  [06:03<00:00,  1.82s/it]\n",
    "# model_name = 'llama3.2:3b-instruct-fp16'  # acc=87.00% [01:02<00:00,  3.19it/s]\n",
    "# model_name = 'qwen3:4b'\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 200,  # max number of tokens to predict\n",
    "    'temperature': 0.0,\n",
    "    'top_p': 0.0,\n",
    "}\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "prompt_template = \"\"\"Consid√©rez l'avis suivant:\n",
    "\n",
    "\"{{text}}\"\n",
    "\n",
    "Est-ce que ce texte exprime globalement un avis positif ou n√©gatif ? R√©pondez seulement par \"positif\" ou \"n√©gatif\" sans donner d'explications.\"\"\"\n",
    "\n",
    "jtemplate = Template(prompt_template)\n",
    "\n",
    "n_correct = 0\n",
    "n=0\n",
    "\n",
    "# We will do that for only the first 20 examples:\n",
    "# tqdm = barre de progression\n",
    "for i in tqdm(range(20)):\n",
    "    sample = ds[i]\n",
    "    opinion_label = \"n√©gatif\" if sample['label'] < 3 else \"positif\"\n",
    "    prompt = jtemplate.render(text=sample['text'])\n",
    "    # print(sample['text'])\n",
    "    result = llmclient.generate(model=model_name, prompt=prompt, options=model_options)\n",
    "    predicted_label = result['response'].lower()\n",
    "    predicted_label = \"\".join(c for c in predicted_label if c.isalpha())\n",
    "    # print(\"LLM response:\\n\", predicted_label)\n",
    "    n += 1\n",
    "    if predicted_label == opinion_label:\n",
    "        n_correct += 1\n",
    "\n",
    "precision = round(100*float(n_correct)/n, 2)\n",
    "\n",
    "print(f\"LLM={model_name}, Classification Precision={precision:.2f}%  ({n_correct}/{n})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aspect-based sentiment analysis in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"Prix\": \"Non exprim√©e\",\n",
      "  \"Cuisine\": \"Positive\",\n",
      "  \"Service\": \"Positive\",\n",
      "  \"Ambiance\": \"N√©gative\",\n",
      "  \"Emplacement\": \"Non exprim√©e\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "from ollama import Client\n",
    "\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 4000,\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "text_avis = \"\"\"Bonne nourriture avec des produits frais\n",
    "Service tr√®s aimable\n",
    "Ambiance sonore √©lev√©e et service long en raison d'un nombre important de convives m√™me en semaine.\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"Consid√©rez l'avis suivant:\n",
    "\n",
    "\"{{text}}\"\n",
    "\n",
    "Quelle est la valeur de l'opinion exprim√©e sur chacun des aspects suivants : Prix, Cuisine, Service, Ambiance et Emplacement?\n",
    "\n",
    "La valeur d'une opinion doit √™tre une des valeurs suivantes: \"Positive\", \"N√©gative\", \"Neutre\", ou \"Non exprim√©e\".\n",
    "La valeur neutre correspond au cas o√π le texte contient √† la fois au moins une opinion positive sur l'aspect et au moins une opinion n√©gative sur le m√™me aspect.\n",
    "\n",
    "La r√©ponse doit se limiter au format json suivant:\n",
    "{ \"Prix\": opinion, \"Cuisine\": opinion, \"Service\": opinion, \"Ambiance\": opinion, \"Emplacement\": opinion}.\n",
    "\n",
    "\"\"\"\n",
    "# /no_think √† la fin du prompt si model avec raisonnement et ne pas utiliser le raisonnement\n",
    "\n",
    "jtemplate = Template(prompt_template)\n",
    "\n",
    "\n",
    "prompt = jtemplate.render(text=text_avis)\n",
    "\n",
    "result = llmclient.generate(model=model_name, prompt=prompt, options=model_options)\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Topic classification in French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'text', 'lang'],\n",
       "    num_rows: 701\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds_train = load_dataset(\"mteb/sib200\", \"fra_Latn\", split='train', download_mode=None)\n",
    "\n",
    "\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'text': \"La Turquie est entour√©e par des mers sur trois c√¥t√©s : la mer √âg√©e √† l'ouest, la mer Noire au nord et la mer M√©diterran√©e au sud.\",\n",
       " 'lang': 'fra_Latn'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(ds_train['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 4,\n",
       " 'text': \"Au d√©but de la guerre, ils voguaient essentiellement sur la mer. Mais au vu des progr√®s r√©alis√©s en mati√®re de radars, et √©tant donn√© leur pr√©cision sans cesse grandissante, les sous-marins √©taient oblig√© de passer sous l'eau pour ne pas √™tre rep√©r√©s.\",\n",
       " 'lang': 'fra_Latn'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete this exercise by writing code to use LLM prompting with ollama in order to classify the texts of the dataset into topics. Compute the accuracy of such classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire sur les 20 premiers exemples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforced structured output with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract structured data from text, we can explcitely instruct the LLM (in the prompt) to produce the requested information in a structured format, for instance json. We then parse the response to isolate the json structure and transform it into a json object.\n",
    "\n",
    "However, instructing the LLM about the output format does not gurantee that the model will always follow the instruction and output the information in the same predefined structured format.\n",
    "\n",
    "To solve this issue, we need to use `structured decoding` or `guided decoding`: we formally define the structure of the output using regular expressions, or context-free grammars, or Json schemas, and then provide this formal definition as an additional argument for generation. During the decoding (generation), a parsing state is maintained, and at each token generation step, only tokens that are compatible with the current parsing state can be sampled. This gurantees that the generation output will strictly match the structure definition.\n",
    "\n",
    "The Ollama Python library offers this guided decoding functionality. Pass in the structure definition to the format argument as a Json schema. The best way to do this is to define Pydantic dataclasses, and pass the Json schema of the main dataclass, obtained with the `model_json_schema()` function.\n",
    "More details at: https://ollama.com/blog/structured-outputs\n",
    "\n",
    "In the following cells, we will show examples of structured decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition (NER) with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we perform LLM-based NER without structured decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\"entity\": \"Korean\", \"type\": \"LOC\"},\n",
      "    {\"entity\": \"Naver Cloud\", \"type\": \"ORG\"},\n",
      "    {\"entity\": \"cloud service provider\", \"type\": \"ORG\"},\n",
      "    {\"entity\": \"Nvidia\", \"type\": \"ORG\"},\n",
      "    {\"entity\": \"AI chip giant\", \"type\": \"ORG\"},\n",
      "    {\"entity\": \"East Asian\", \"type\": \"LOC\"},\n",
      "    {\"entity\": \"California\", \"type\": \"LOC\"},\n",
      "    {\"entity\": \"GTC 2025\", \"type\": \"EVENT\"},\n",
      "    {\"entity\": \"San Jose\", \"type\": \"LOC\"},\n",
      "    {\"entity\": \"Thursday\", \"type\": \"DATE\"},\n",
      "    {\"entity\": \"Naver Cloud CEO Kim Yu-won\", \"type\": \"PERSON\"}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from jinja2 import Template\n",
    "from ollama import Client\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 400,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"Definition: A named entity mention is either a name that refers to an entity like a person (PERSON), organization (ORG), location (LOC) or an event (EVENT), or an expression denoting a date (DATE) or an amount of money (MONEY).\n",
    "\n",
    "Extract all named entity mentions from the following text. The result should be a json list where each element is a json object of the form: {\"entity\": \"the extracted named entity\", \"type\": \"the type of the entity\"}:\n",
    "\n",
    "Text: \"{{text}}\"\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"Korean cloud service provider Naver Cloud has partnered with AI chip giant Nvidia to establish a localized AI system targeting the East Asian market and expects to deliver ‚Äútangible results‚Äù within the year. The announcement was made at Nvidia's GTC 2025 in San Jose, California, during a keynote speech by Naver Cloud CEO Kim Yu-won on Thursday.\"\"\"\n",
    "\n",
    "jtemplate = Template(prompt_template)\n",
    "\n",
    "prompt = jtemplate.render(text=text)\n",
    "\n",
    "result = llmclient.generate(model=model_name, prompt=prompt, options=model_options)\n",
    "\n",
    "print(result.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but with structured decoding to enforce the json structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output string (guranteed to be a valid json string):\n",
      " {\n",
      "    \"entities\": [\n",
      "        {\"entity\": \"Korean\", \"type\": \"LOC\"},\n",
      "        {\"entity\": \"Naver Cloud\", \"type\": \"ORG\"},\n",
      "        {\"entity\": \"cloud\", \"type\": \"LOC\"}, {\"entity\": \"Nvidia\", \"type\": \"ORG\"},\n",
      "        {\"entity\": \"AI chip\", \"type\": \"MONEY\"}, {\"entity\": \"California\", \"type\": \"LOC\"},\n",
      "        {\"entity\": \"GTC 2025\", \"type\": \"EVENT\"},\n",
      "        {\"entity\": \"San Jose\", \"type\": \"LOC\"},\n",
      "        {\"entity\": \"Thursday\", \"type\": \"DATE\"},\n",
      "        {\"entity\": \"Naver Cloud CEO Kim Yu-won\", \"type\": \"PERSON\"}\n",
      "    ]\n",
      "}\n",
      "Python object:\n",
      "[NamedEntity(entity='Korean', type='LOC'),\n",
      " NamedEntity(entity='Naver Cloud', type='ORG'),\n",
      " NamedEntity(entity='cloud', type='LOC'),\n",
      " NamedEntity(entity='Nvidia', type='ORG'),\n",
      " NamedEntity(entity='AI chip', type='MONEY'),\n",
      " NamedEntity(entity='California', type='LOC'),\n",
      " NamedEntity(entity='GTC 2025', type='EVENT'),\n",
      " NamedEntity(entity='San Jose', type='LOC'),\n",
      " NamedEntity(entity='Thursday', type='DATE'),\n",
      " NamedEntity(entity='Naver Cloud CEO Kim Yu-won', type='PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from jinja2 import Template\n",
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "class NamedEntity(BaseModel):\n",
    "  entity: str\n",
    "  type: Literal[\"PERSON\", \"ORG\", \"LOC\", \"DATE\", \"EVENT\", \"MONEY\"]\n",
    "  \n",
    "\n",
    "class NamedEntities(BaseModel):\n",
    "  entities: list[NamedEntity]\n",
    "\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 400,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"Definition: A named entity mention is either a name that refers to an entity like a person (PERSON), organization (ORG), location (LOC) or an event (EVENT), or an expression denoting a date (DATE) or an amount of money (MONEY).\n",
    "\n",
    "Extract all named entity mentions from the following text. The result should be a json list where each element is a json object of the form: {\"entity\": \"the extracted named entity\", \"type\": \"the type of the entity\"}:\n",
    "\n",
    "Text: \"{{text}}\"\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"Korean cloud service provider Naver Cloud has partnered with AI chip giant Nvidia to establish a localized AI system targeting the East Asian market and expects to deliver ‚Äútangible results‚Äù within the year. The announcement was made at Nvidia‚Äôs GTC 2025 in San Jose, California, during a keynote speech by Naver Cloud CEO Kim Yu-won on Thursday.\"\"\"\n",
    "\n",
    "jtemplate = Template(prompt_template)\n",
    "\n",
    "prompt = jtemplate.render(text=text)\n",
    "\n",
    "result = llmclient.generate(model=model_name, prompt=prompt, options=model_options, format=NamedEntities.model_json_schema())\n",
    "\n",
    "print(\"LLM output string (guranteed to be a valid json string):\\n\", result.response)\n",
    "\n",
    "# We can automatically tranform the json string output produced by the LLM into a \n",
    "# Python object of class NamedEntities defined above\n",
    "output_object = NamedEntities.model_validate_json(result.response)\n",
    "\n",
    "print(\"Python object:\")\n",
    "pprint(output_object.entities, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output string (guranteed to be a valid json string):\n",
      " {\n",
      "    \"entities\": [\n",
      "        {\"entity\": \"Korean\", \"type\": \"LOC\"},\n",
      "        {\"entity\": \"Naver Cloud\", \"type\": \"ORG\"},\n",
      "        {\"entity\": \"cloud\", \"type\": \"LOC\"}, {\"entity\": \"Nvidia\", \"type\": \"ORG\"},\n",
      "        {\"entity\": \"AI chip\", \"type\": \"MONEY\"}, {\"entity\": \"California\", \"type\": \"LOC\"},\n",
      "        {\"entity\": \"GTC 2025\", \"type\": \"EVENT\"},\n",
      "        {\"entity\": \"San Jose\", \"type\": \"LOC\"},\n",
      "        {\"entity\": \"Thursday\", \"type\": \"DATE\"},\n",
      "        {\"entity\": \"Naver Cloud CEO Kim Yu-won\", \"type\": \"PERSON\"}\n",
      "    ]\n",
      "}\n",
      "Python object:\n",
      "[NamedEntity(entity='Korean', type='LOC'),\n",
      " NamedEntity(entity='Naver Cloud', type='ORG'),\n",
      " NamedEntity(entity='cloud', type='LOC'),\n",
      " NamedEntity(entity='Nvidia', type='ORG'),\n",
      " NamedEntity(entity='AI chip', type='MONEY'),\n",
      " NamedEntity(entity='California', type='LOC'),\n",
      " NamedEntity(entity='GTC 2025', type='EVENT'),\n",
      " NamedEntity(entity='San Jose', type='LOC'),\n",
      " NamedEntity(entity='Thursday', type='DATE'),\n",
      " NamedEntity(entity='Naver Cloud CEO Kim Yu-won', type='PERSON')]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from jinja2 import Template\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "class NamedEntity(BaseModel):\n",
    "  entity: str\n",
    "  type: Literal[\"PERSON\", \"ORG\", \"LOC\", \"DATE\", \"EVENT\", \"MONEY\"]\n",
    "  \n",
    "\n",
    "class NamedEntities(BaseModel):\n",
    "  entities: list[NamedEntity]\n",
    "\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434/v1'\n",
    "\n",
    "llmclient = OpenAI(base_url=ollama_url, api_key=\"EMPTY\")\n",
    "\n",
    "prompt_template = \"\"\"Definition: A named entity mention is either a name that refers to an entity like a person (PERSON), organization (ORG), location (LOC) or an event (EVENT), or an expression denoting a date (DATE) or an amount of money (MONEY).\n",
    "\n",
    "Extract all named entity mentions from the following text. The result should be a json list where each element is a json object of the form: {\"entity\": \"the extracted named entity\", \"type\": \"the type of the entity\"}:\n",
    "\n",
    "Text: \"{{text}}\"\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"Korean cloud service provider Naver Cloud has partnered with AI chip giant Nvidia to establish a localized AI system targeting the East Asian market and expects to deliver ‚Äútangible results‚Äù within the year. The announcement was made at Nvidia‚Äôs GTC 2025 in San Jose, California, during a keynote speech by Naver Cloud CEO Kim Yu-won on Thursday.\"\"\"\n",
    "\n",
    "jtemplate = Template(prompt_template)\n",
    "\n",
    "prompt = jtemplate.render(text=text)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "result = llmclient.chat.completions.parse(\n",
    "    model=model_name, \n",
    "    messages= messages,\n",
    "    max_tokens=400,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    response_format=NamedEntities)\n",
    "\n",
    "print(\"LLM output string (guranteed to be a valid json string):\\n\", result.choices[0].message.content)\n",
    "\n",
    "# We can automatically tranform the json string output produced by the LLM into a \n",
    "# Python object of class NamedEntities defined above\n",
    "output_object = NamedEntities.model_validate_json(result.choices[0].message.content)\n",
    "\n",
    "print(\"Python object:\")\n",
    "pprint(output_object.entities, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if we don't explicitly instruct the LLM to extract named entities and we use as prompt only the text (without any instruction), the output will still contain extracted named entities. This is because with the enforced structured decoding, the decoding algorithm is forced to output tokens compatible with the defined json structure, and therefore, is forced to output the json keys `entities:`, `entity:`and `type:`, hence pushing for the selection of entity tokens. However, without the explicit instructions and entity deiniftions, the NER performance will probably be lower. See the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM output string (guranteed to be a valid json string):\n",
      " { \"entities\": [ { \"entity\": \"Naver Cloud\", \"type\": \"ORG\" }, { \"entity\": \"Nvidia\", \"type\": \"ORG\" }, { \"entity\": \"AI system\", \"type\": \"ORG\" }, { \"entity\": \"East Asian market\", \"type\": \"LOC\" }, { \"entity\": \"GTC 2025\", \"type\": \"EVENT\" }, { \"entity\": \"San Jose, California\", \"type\": \"LOC\" } ] }\n",
      "\n",
      "\t  \t\t\t  \t\t\t  \t\t\t\t\t\t\t\n",
      "Python object:\n",
      "[NamedEntity(entity='Naver Cloud', type='ORG'),\n",
      " NamedEntity(entity='Nvidia', type='ORG'),\n",
      " NamedEntity(entity='AI system', type='ORG'),\n",
      " NamedEntity(entity='East Asian market', type='LOC'),\n",
      " NamedEntity(entity='GTC 2025', type='EVENT'),\n",
      " NamedEntity(entity='San Jose, California', type='LOC')]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class NamedEntity(BaseModel):\n",
    "  entity: str\n",
    "  type: Literal[\"PERSON\", \"ORG\", \"LOC\", \"DATE\", \"EVENT\", \"MONEY\"]\n",
    "  \n",
    "\n",
    "class NamedEntities(BaseModel):\n",
    "  entities: list[NamedEntity]\n",
    "\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 400,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "text = \"\"\"Korean cloud service provider Naver Cloud has partnered with AI chip giant Nvidia to establish a localized AI system targeting the East Asian market and expects to deliver ‚Äútangible results‚Äù within the year. The announcement was made at Nvidia‚Äôs GTC 2025 in San Jose, California, during a keynote speech by Naver Cloud CEO Kim Yu-won on Thursday.\"\"\"\n",
    "\n",
    "\n",
    "result = llmclient.generate(model=model_name, prompt=text, options=model_options, format=NamedEntities.model_json_schema())\n",
    "\n",
    "print(\"LLM output string (guranteed to be a valid json string):\\n\", result.response)\n",
    "\n",
    "# We can automatically tranform the json string output produced by the LLM into a \n",
    "# Python object of class NamedEntities defined above\n",
    "output_object = NamedEntities.model_validate_json(result.response)\n",
    "\n",
    "print(\"Python object:\")\n",
    "pprint(output_object.entities, compact=True, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that the choice of the structure format is defined by the dataclass definition (names of its felds/attributes and the types of values they take). This structure definition has an impact on the LLM task performance. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedEntities(person_names_mentioned=['Kim Yu-won'], organization_names_mentioned=['Naver Cloud', 'Nvidia'], location_names_mentioned=['San Jose, California', 'East Asian market'])\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "class NamedEntities(BaseModel):\n",
    "  person_names_mentioned: list[str]\n",
    "  organization_names_mentioned: list[str]\n",
    "  location_names_mentioned: list[str]\n",
    "\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 400,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "text = \"\"\"Selon la diplomatie turque, une s√©rie de r√©unions trilat√©rales est pr√©vue entre les Etats-Unis, la Turquie et l‚ÄôUkraine, ainsi qu‚Äôentre les deux premiers nomm√©s et la Russie. Volodymyr Zelensky a d√©cid√© de ne pas prendre part aux n√©gociations apr√®s avoir appris que son homologue russe, Vladimir Poutine, ne sera pas pr√©sent.\n",
    "\"\"\"\n",
    "\n",
    "text = \"\"\"Korean cloud service provider Naver Cloud has partnered with AI chip giant Nvidia to establish a localized AI system targeting the East Asian market and expects to deliver ‚Äútangible results‚Äù within the year. The announcement was made at Nvidia‚Äôs GTC 2025 in San Jose, California, during a keynote speech by Naver Cloud CEO Kim Yu-won on Thursday.\"\"\"\n",
    "\n",
    "response = llmclient.chat(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': text,\n",
    "    }\n",
    "  ],\n",
    "  model=model_name,\n",
    "  format=NamedEntities.model_json_schema(),\n",
    ")\n",
    "\n",
    "entities = NamedEntities.model_validate_json(response.message.content)\n",
    "pprint(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example without any explicit instruction, this time in French:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NamedEntities(person_names_mentioned=['Volodymyr Zelensky', 'Vladimir Poutine'], organization_names_mentioned=['Turquie', '√âtats-Unis', 'Ukraine', 'Russie'], location_names_mentioned=[])\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from pydantic import BaseModel\n",
    "from pprint import pprint\n",
    "\n",
    "class NamedEntities(BaseModel):\n",
    "  person_names_mentioned: list[str]\n",
    "  organization_names_mentioned: list[str]\n",
    "  location_names_mentioned: list[str]\n",
    "\n",
    "\n",
    "model_name = 'qwen2.5:7b'\n",
    "ollama_url = 'http://localhost:11434'\n",
    "\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "model_options = {\n",
    "    'num_predict': 400,  # max number of tokens to predict\n",
    "    'temperature': 0.1,\n",
    "    'top_p': 0.9,\n",
    "}\n",
    "\n",
    "text = \"\"\"Selon la diplomatie turque, une s√©rie de r√©unions trilat√©rales est pr√©vue entre les Etats-Unis, la Turquie et l‚ÄôUkraine, ainsi qu‚Äôentre les deux premiers nomm√©s et la Russie. Volodymyr Zelensky a d√©cid√© de ne pas prendre part aux n√©gociations apr√®s avoir appris que son homologue russe, Vladimir Poutine, ne sera pas pr√©sent.\n",
    "\"\"\"\n",
    "\n",
    "response = llmclient.chat(\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': text,\n",
    "    }\n",
    "  ],\n",
    "  model=model_name,\n",
    "  format=NamedEntities.model_json_schema(),\n",
    ")\n",
    "\n",
    "entities = NamedEntities.model_validate_json(response.message.content)\n",
    "pprint(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of named entities and their attributes/properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can push further this idea of enforcing the generation of pre-defined structures during LLM decoding, for instance to extract not only entities but also attributes or properties of the entities, as in this example (you can try and add other attributes/properties by adding corresponding fields in the Country dataclass):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model raw response:\n",
      " {\n",
      "\"name\": \"Canada\",\n",
      "\"capital\": \"Ottawa\",\n",
      "\"languages\": [\"English\", \"French\"],\n",
      "\"prime_minister\": \"Justin Trudeau\",\n",
      "\"population_in_millions\": 40,\n",
      "\"surface_in_km2\": 9984670\n",
      "}\n",
      "\n",
      " \t \t \t \t \t \t \t \t \t \t\n",
      "Structured output:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Country(name='Canada', capital='Ottawa', languages=['English', 'French'], prime_minister='Justin Trudeau', population_in_millions=40, surface_in_km2=9984670)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from ollama import Client\n",
    "\n",
    "# We assume that ollama has been installed and the ollama server already started (see https://ollama.com/)\n",
    "ollama_url = 'http://localhost:11434'\n",
    "llmclient = Client(host=ollama_url)\n",
    "\n",
    "class Country(BaseModel):\n",
    "  name: str\n",
    "  capital: str\n",
    "  languages: list[str]\n",
    "  prime_minister: str\n",
    "  population_in_millions: int\n",
    "  surface_in_km2: int\n",
    "\n",
    "model_name = 'gemma3:4b'\n",
    "\n",
    "response = llmclient.chat(\n",
    "  messages=[ {'role': 'user', 'content': 'Tell me about Canada.'} ],\n",
    "  model=model_name,\n",
    "  format=Country.model_json_schema(),\n",
    "  options={'temperature': 0.5},  # Set temperature to 0 for more deterministic output\n",
    ")\n",
    "\n",
    "print(\"Model raw response:\\n\", response.message.content)\n",
    "\n",
    "print(\"Structured output:\")\n",
    "country = Country.model_validate_json(response.message.content)\n",
    "country"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
